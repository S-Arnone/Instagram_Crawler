{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-Arnone/Instagram_Crawler/blob/main/Instagram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDt4G3hZCp9J"
      },
      "outputs": [],
      "source": [
        "# @author:          Samuel Arnone-Roller\n",
        "# @email:           RollerSa@uw.edu\n",
        "# @website:         https://hgis.uw.edu\n",
        "# @organization:    University of Washington - Graduate Student\n",
        "# @description:     An Instagram crawler developed for GEOG 595."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9dXwu7Ay9Twv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2114a074-bea0-4652-a2ab-f9040600c29a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 34.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51 kB 31.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 57 kB 5.7 MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |█████▉                          | 10 kB 31.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 20 kB 38.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 30 kB 44.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 40 kB 49.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 51 kB 51.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 4.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing Kora to the remote google colab server. Kora is a collection of tools to make programming on Google Colab easier.\n",
        "!pip install kora -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fxx5mNyHC0-0"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import time, datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "from kora.selenium import wd as bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CSErhAJ1C50o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efc54c9-f395-4e31-e12e-9a6cf68991e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it had an svg for a child\n",
            "it did not have svg for a child\n",
            "it had an svg for a child\n",
            "it had an svg for a child\n",
            "it did not have svg for a child\n",
            "it had an svg for a child\n",
            "it had an svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it had an svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it had an svg for a child\n",
            "it had an svg for a child\n",
            "it had an svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it had an svg for a child\n",
            "it had an svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it did not have svg for a child\n",
            "it had an svg for a child\n",
            "it had an svg for a child\n",
            "it had an svg for a child\n",
            "it had an svg for a child\n",
            "the crawling task is finished.\n"
          ]
        }
      ],
      "source": [
        "# URL Selection and Manipulation\n",
        "## URL Variable can be manually altered by the user to collect data on different subject matters. In order to obtain a valid url for this bot, you must\n",
        "## first manually download the HTML of the entries you are looking to crawl, this creates some limitations specifically in limiting the way that your bot\n",
        "## can interact with React JS integrated HTML, but allows for ease of access without an API. After you download the HTML, upload it to your google drive\n",
        "## and copy the link which is presented when you click 'download'. This may be somewhat tricky to access, but is possible. This download link will be accessed\n",
        "## by the bot as a means to download the HTML directly to the Colab files. If this does not work, as it is sometimes error prone, you may wish to manually\n",
        "## upload the HTML file in the files section of Colab located on the left hand side of the screen - Note: should you do this you need to comment out\n",
        "## bot.get(url)\n",
        "url = \"https://drive.google.com/u/0/uc?id=1zXefaq9291x_nGBu7v-s0OLIhuGFz2IX&export=download\"\n",
        "\n",
        "##Input the targeting url to the bot, and the bot will load data from the url.\n",
        "#bot.get(url)\n",
        "\n",
        "# Global Values\n",
        "## An array to store all post urls.\n",
        "media_urls = []\n",
        "## An array to store the retrieved results.\n",
        "results = []\n",
        "\n",
        "\n",
        "# Reading in the HTML\n",
        "## The following code will interact with the HTML you have downloaded, either through the bot or manually, so as to make it accessable to BeautifulSoup\n",
        "HtmlFile = open('/content/Raqqa.html', 'r', encoding='utf-8')\n",
        "source_code = HtmlFile.read() \n",
        "\n",
        "# BeautifulSoup\n",
        "## Beautiful soup will now parse the HTML DOM (Document Object Model) so that it is readable and workable for the following code - simple but essential.\n",
        "soup = BeautifulSoup(source_code, 'html.parser')\n",
        "\n",
        "# Finding Posts\n",
        "## Every post on instagram is contained under the div class listed below. This means that the bot must isolate those posts in advance of crawling\n",
        "## the information you are interested in. The following code does exactly that, before initiating information crawling.\n",
        "\n",
        "posts = soup.find_all('div', class_=\"v1Nh3 kIKUG _bz0w\")\n",
        "\n",
        "# Crawling Posts\n",
        "## Using a for loop for all posts contained in your HTML, the following will run through a series of extractions by selectively\n",
        "## looking for standardized information patterns.\n",
        "for content in posts:\n",
        "\n",
        "    # Video Filtering\n",
        "    ## The following allows for you to track the progress of processing and monitor for errors while isolating photos from videos.\n",
        "    ## Videos, having an svg, can be isolated by this child and be filtered from the final data.\n",
        "    if content.findChild(\"svg\"):\n",
        "      print(\"it had an svg for a child\")\n",
        "    else:\n",
        "      print(\"it did not have svg for a child\")\n",
        "      try:\n",
        "        \n",
        "        # Post URL\n",
        "        ## This pulls the post url contained within the image information of each instagram post. This is necessary for work verification by\n",
        "        ## yourself or others. Such information is contained in the anchor element of each post as an href.\n",
        "        post_url =  content.find(\"a\").attrs[\"href\"]\n",
        "\n",
        "        # Username Location\n",
        "        ## The following picks through the image alt attached to each post, extracting the username of each poster by looking between the\n",
        "        ## common terms \"by \" and \" on\" or \" in\". The origin of this information is in the alt text of post images.\n",
        "        username = content.find(\"img\").attrs[\"alt\"]\n",
        "        start_U = username.find(\"by \") + len(\"by \")\n",
        "        if \" on\" in username:\n",
        "          end_U = username.find(\" on \")\n",
        "        else:\n",
        "          end_U = username.find(\" in \")\n",
        "        substring_U = username[start_U:end_U]\n",
        "\n",
        "        # Creation Time\n",
        "        ## Drawing on the method used to extract usernames, the following extracts the time of posting by extracting between \"on \" and \". \"\n",
        "        created_on = content.find(\"img\").attrs[\"alt\"]\n",
        "        if \"on \" in created_on:\n",
        "          start_C = created_on.find(\"on \") + len(\". \")\n",
        "        else:\n",
        "          start_C = created_on.find(\"in \") + len(\". \")\n",
        "        end_C = created_on.find(\". \")\n",
        "        substring_C = created_on[start_C:end_C]\n",
        "        \n",
        "        # Alt Text Description\n",
        "        ## The same method can also be used to extract the image alt in lieu of caption availability. This describes what is present\n",
        "        ## in the image associated with each post. It is not perfect, at worst it can provide simple observations,\n",
        "        ## but at best it can replicate text and indentify important visual data like flag types.\n",
        "        Alt_Text = content.find(\"img\").attrs[\"alt\"]\n",
        "        if \"May be\" in Alt_Text:\n",
        "          start_A = Alt_Text.find(\"May be\") + len(\"May be\")\n",
        "          end_A = len(Alt_Text)\n",
        "        substring_A = Alt_Text[start_A:end_A]\n",
        "\n",
        "        # DEPRICATED: Post Likes\n",
        "        ## As this bot works with manually downloaded HTML, this feature has been depricated but remains accessible for those interested\n",
        "        ## in retooling the bot for HTML extraction *by* the bot directly from Instagram. Likes are inaccessible in this model\n",
        "        ## due to the fact that the span containing like information is hidden behind a 'when hover' feature of React JS.\n",
        "        ###LikesDiv = content.find('div', class_=\"_7UhW9 vy6Bb qyrsm h_zdq uL8Hv T0kll\")\n",
        "        ###Likes = LikesDiv.find('span')\n",
        "\n",
        "        # Time of Crawling\n",
        "        ## Collecting the date and time of my own capture allows for later validation of work,\n",
        "        ## additionally, it can allow validation of the time taken to collect data.\n",
        "        collected_at = datetime.datetime.now()\n",
        "\n",
        "        # Data Organization\n",
        "        ## The following organizes our data for transformation into a CSV, assigning row names to data.\n",
        "        row = {'post_url': post_url,\n",
        "                      'username': substring_U,\n",
        "                      'created_on': substring_C,\n",
        "                      'Alt_Text': substring_A,\n",
        "                      'collected_at': collected_at}\n",
        "\n",
        "        # Data Integrity\n",
        "        ## This simply ensures that the same post will not be crawled twice.\n",
        "        if post_url in media_urls:\n",
        "                  print(\"this post has already been added.\")\n",
        "        else:\n",
        "                  results.append(row)\n",
        "                  media_urls.append(post_url)\n",
        "\n",
        "      except:\n",
        "        pass\n",
        "    \n",
        "\n",
        "# DEPRICATED: Bot Work Mediation\n",
        "## The following are unnecessary when we are working with manually downloaded HTML, but in the event that you wish to retool this\n",
        "## code for direct crawling, you will need to use and improve the following code to evade detection as a crawler.\n",
        "## The following makes the bot process posts at a slower rate, like a human being might.\n",
        "# time.sleep(7)\n",
        "## The following will need to be used to scroll down an instagram page so that posts beyond what initially loads can be crawled.\n",
        "# bot.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "## The following will finally be used to tell the bot that it can be finished crawling the Instagram page.\n",
        "#bot.close()\n",
        "\n",
        "# Data storage\n",
        "## Store the results as a pandas dataframe\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Notification of Work Completion\n",
        "## notify the completion of the crawling in the console.\n",
        "print(\"the crawling task is finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual DF Verification\n",
        "## The following is used to print what has been stored in your dataframe, so that you can check for issues before translating your data into\n",
        "## a CSV and dowloading it. Warning: if you are working with a large volume of data, it may be better to conduct work verification in\n",
        "## an excel CSV file due to it's superior readability.\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNQoMn9aoSNc",
        "outputId": "7afd74f4-eaa8-4576-fcd1-9adc979ca06b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    post_url                        username  \\\n",
            "0   https://www.instagram.com/p/CabZjJlowqr/                      شباب الرقه   \n",
            "1   https://www.instagram.com/p/CaSyH14r2tw/                      شباب الرقه   \n",
            "2   https://www.instagram.com/p/CabeLkbo17A/                      شباب الرقه   \n",
            "3   https://www.instagram.com/p/CZwaT_oog27/                      شباب الرقه   \n",
            "4   https://www.instagram.com/p/CalRCSaIgrL/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "5   https://www.instagram.com/p/CalP1F4otv8/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "6   https://www.instagram.com/p/CalPvspIsU0/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "7   https://www.instagram.com/p/CalPf7Po5bH/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "8   https://www.instagram.com/p/CalPUyCIFtV/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "9   https://www.instagram.com/p/CalPPiSI7k2/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "10  https://www.instagram.com/p/CalOKC1I7fU/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "11  https://www.instagram.com/p/CalOE31oo2f/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "12  https://www.instagram.com/p/CalN8fnIXaF/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "13  https://www.instagram.com/p/CalNcUKIFH5/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "14  https://www.instagram.com/p/CalLzWyoPSI/  سوق صباح الاحمد سوق مدينة صباح   \n",
            "15  https://www.instagram.com/p/CalKJLoIWcj/      ديكورات بديل رخام بديل خشب   \n",
            "16  https://www.instagram.com/p/CalKGl0IVww/      ديكورات بديل رخام بديل خشب   \n",
            "17  https://www.instagram.com/p/CalKCmkITuS/      ديكورات بديل رخام بديل خشب   \n",
            "\n",
            "                     created_on  \\\n",
            "0             February 25, 2022   \n",
            "1             February 22, 2022   \n",
            "2             February 25, 2022   \n",
            "3             February 09, 2022   \n",
            "4    Sabah Al-Ahmed صباح الأحمد   \n",
            "5    Sabah Al-Ahmed صباح الأحمد   \n",
            "6    Sabah Al-Ahmed صباح الأحمد   \n",
            "7    Sabah Al-Ahmed صباح الأحمد   \n",
            "8                   صباح الاحمد   \n",
            "9                   صباح الاحمد   \n",
            "10               March 01, 2022   \n",
            "11                  صباح الاحمد   \n",
            "12   Sabah Al-Ahmed صباح الأحمد   \n",
            "13               March 01, 2022   \n",
            "14   Sabah Al-Ahmed صباح الأحمد   \n",
            "15               March 01, 2022   \n",
            "16               March 01, 2022   \n",
            "17               March 01, 2022   \n",
            "\n",
            "                                             Alt_Text  \\\n",
            "0    an image of ‎one or more people and ‎text tha...   \n",
            "1                     an image of one or more people.   \n",
            "2            an image of one or more people and text.   \n",
            "3            an image of one or more people and text.   \n",
            "4    an image of ‎text that says '‎الحساب برعاية -...   \n",
            "5    an image of ‎one or more people and ‎text tha...   \n",
            "6    an image of ‎text that says '‎രട 3/3 الس 4/3 ...   \n",
            "7    an image of ‎indoor and ‎text that says '‎S س...   \n",
            "8    an image of ‎bicycle and ‎text that says '‎AB...   \n",
            "9                                    an image of cat.   \n",
            "10                                                      \n",
            "11   an image of ‎text that says '‎1 أكاريمية الطف...   \n",
            "12   an image of ‎text that says '‎الحساب برعاية -...   \n",
            "13                                                      \n",
            "14   an image of ‎text that says '‎الحساب برعاية -...   \n",
            "15   an image of ‎indoor and ‎text that says '‎511...   \n",
            "16            an image of furniture, indoor and text.   \n",
            "17   an image of ‎furniture, indoor and ‎text that...   \n",
            "\n",
            "                 collected_at  \n",
            "0  2022-03-07 17:35:33.984488  \n",
            "1  2022-03-07 17:35:33.987639  \n",
            "2  2022-03-07 17:35:33.989295  \n",
            "3  2022-03-07 17:35:33.989924  \n",
            "4  2022-03-07 17:35:33.990610  \n",
            "5  2022-03-07 17:35:33.991583  \n",
            "6  2022-03-07 17:35:33.992044  \n",
            "7  2022-03-07 17:35:33.992493  \n",
            "8  2022-03-07 17:35:33.992932  \n",
            "9  2022-03-07 17:35:33.993451  \n",
            "10 2022-03-07 17:35:33.994853  \n",
            "11 2022-03-07 17:35:33.995532  \n",
            "12 2022-03-07 17:35:33.996214  \n",
            "13 2022-03-07 17:35:33.996770  \n",
            "14 2022-03-07 17:35:33.997966  \n",
            "15 2022-03-07 17:35:33.998624  \n",
            "16 2022-03-07 17:35:33.999295  \n",
            "17 2022-03-07 17:35:33.999940  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxJng3mfXT3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e611b2e6-5244-4629-c1b9-1ae02ebd50b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "# Enable Google Colab to Access Your Drive\n",
        "from google.colab import drive\n",
        "# Mount your Drive to the Colab\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Name the Output and its Location\n",
        "output_file = '/gdrive/My Drive/CrawlerDataWIP.csv'\n",
        "\n",
        "# Initiate Data Save as CSV\n",
        "df.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3-zJPqIRb7w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ba021b8-6b2c-44bb-c2f7-fa1f1227d9d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0125eaa8-4132-4d66-b4d2-6b968639b311\", \"CrawlerDataWIP.csv\", 6385)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the csv has been downloaded to your local computer. The program has been completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Downloading! \n",
        "## This final step initiates CSV downloading and notifies you on completion.\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Instagram.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}